{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Profile Details:\n",
      "Full name: Izzy Hibbert\n",
      "Headline: Talent Acquisition Specialist\n",
      "Location: London Area, United Kingdom\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Read the HTML content from the file\n",
    "with open('html5.txt', 'r', encoding='utf-8') as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# Extract full name\n",
    "full_name_match = re.search(r'(?<=<title>).+? \\| LinkedIn', html_content)\n",
    "full_name = full_name_match.group().replace(\" | LinkedIn\", \"\") if full_name_match else \"Full name not found\"\n",
    "\n",
    "# Extract headline\n",
    "headline_match = re.search(r'<div class=\"text-body-medium break-words\"[^>]*>([^<]+)</div>', html_content)\n",
    "headline = headline_match.group(1).strip() if headline_match else \"Headline not found\"\n",
    "\n",
    "# Extract location\n",
    "location_match = re.search(r'<span class=\"text-body-small inline t-black--light break-words\"[^>]*>([^<]+)</span>', html_content)\n",
    "location = location_match.group(1).strip() if location_match else \"Location not found\"\n",
    "\n",
    "# Print the extracted details\n",
    "print(\"Extracted Profile Details:\")\n",
    "print(f\"Full name: {full_name}\")\n",
    "print(f\"Headline: {headline}\")\n",
    "print(f\"Location: {location}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position: Data Recruitment Consultant (Nordicsüá©üá∞üêø) at Evolution Recruitment Solutions - We get your projects delivered through the provision of Tech Freelancers | Evolution Exchange Podcast Host\n",
      "Time Period: Highlights\n",
      "About: Evolution Nordics\n",
      "Skills: ['Highlights', 'Sian viewed your profile in the past 90 days', 'Reach out to Sian to understand their buying needs.', 'Free insight from Sales Navigator', 'About', 'I connect exciting Nordics tech companies with world-leading freelance talent. Underpinning my talent acquisition strategy are three main pillars:<!----><br><br><!---->Contribution ‚Äì I speak to Tech Leaders every day about the challenges they face and the insights they‚Äôve gained from the market ‚Äì insights that, up until now, were left unshared. Through my podcast platform, I‚Äôm sitting down with business leaders and sharing these insights with my network of freelancers. Check out my featured section for a sample of my latest content.<!----><br><span class=\"white-space-pre\"> </span><!----><!----><br><!---->Community ‚Äì I‚Äôm proud to host the Evolution Exchange Podcast Nordics ‚Äì one of the Nordics fastest growing tech podcasts. With well over 250 daily listeners, the Evolution Exchange Podcast Nordics is a hive of discussion where the best companies and freelancers come together to exchange tips, tricks and advice.<!----><br><br><!---->Collaboration ‚Äì I work with an incredible team of talented and ambitious recruiters who are specialists in sourcing people for agile development teams.<span class=\"white-space-pre\"> </span><br><br><!---->If you would like to contribute your insights with our growing community of Copenhagen based freelancers, or are looking for your next assignment, click connect.', 'Featured', 'Document', 'Our recruitment services', 'A more in-depth overview of the services we provide for our clients in Denmark', 'Post', 'Link', 'Evo Nordics #322 - How To Freelance In Denmark', 'Spotify', 'Activity', '2,272 followers', 'Experience', 'Evolution Nordics', '2 yrs', 'Senior Recruitment Consultant', 'Full-time', 'Warrington, England, United Kingdom', 'Consistent Recruitment Consultant<span class=\"white-space-pre\"> </span></span><span class=\"visually-hidden\"><!---->Consistent Recruitment Consultant<span class=\"white-space-pre\"> </span></span>\\n          </div>\\n      \\n      </div>\\n  \\n    </div>\\n  \\n<!----><!----><!---->            </div>\\n<!---->              <span class=\"t-14 t-normal\\n                  t-black--light\">\\n                <span class=\"pvs-entity__caption-wrapper\" aria-hidden=\"true\"><!---->Aug 2023 - Dec 2023 ¬∑ 5 mos', 'Recruitment Consultant', 'Auxiliare de conversaci√≥n', 'British Council ¬∑ Full-time', 'Murcia, Regi√≥n de Murcia, Spain', 'Customer Service Assistant', 'Iceland Foods ¬∑ Part-time', 'Liverpool, England, United Kingdom', 'Bartender', 'Liverpool Football Club ¬∑ Part-time', 'Liverpool, England, United Kingdom', 'Customer Service Staff', 'Mauds Ice Creams Ltd', 'Newtownards, County Down, Northern Ireland', 'Education', 'University of Liverpool', 'Geography BSc, Geography', 'Grade: 1st class honours', 'Activities and societies: Member of the hockey team and geography society. I also play netball in the campus league.', 'Regent House Grammar School', 'Grade: 10 GCSEs (A*-C) 3 A-levels (BCC)', 'Activities and societies: Hockey team, golf team, school council, learning mentor, year 8 mentor, prefect.', 'A* in maths and A in English at GCSE', 'Licenses &amp; certifications', 'Teaching English as a Foreign Language (TEFL)', 'University of Cambridge', 'Driver', 'Driver and Vehicle Licensing Agency (DVLA)', 'Volunteering', 'Volunteer group in school', 'Habitat for Humanity', 'Poverty Alleviation', 'A group of students in my grammar school and a group from another came together to learn about this great organisation. Together we improved a church in our community by working as a team to clean up, paint and plaster. We also had a task of raising ¬£1,000 which we did through different things within school such as a paid non uniform day, a talent show and a bun sale.', 'Assistant', 'Donaghadee Primary School', 'Education', 'A week long work experience as a classroom assistant with Year 2s (5/6 year olds). Marking work, teaching them a couple of things from A level languages, helping the more challenged children and staying behind to help clean up and help wherever needed throughout the school. I was also put on playground duty at break and lunch.', 'Skills', 'Geographic Information Systems (GIS)', 'Endorsed by 1 people in the last 6 months', '4 endorsements', 'Customer Service', 'Endorsed by 1 people in the last 6 months', '3 endorsements', 'Recommendations', 'Received', 'Given', 'Troy (Kyung Un) Choi', '¬∑ 2nd', 'Freelance Cloud Data Solution &amp; Architecture', 'I highly recommend Sian Vance as a recruitment consultant for anyone seeking new opportunities in the tech industry. Sian helped me secure a new project at TDC Net and I had a wonderful experience throughout the process thanks to her expert advice and support. Her knowledge of the industry, attention to detail, and excellent communication skills make her an invaluable asset to anyone looking for a new role. I would not hesitate to recommend Sian to other freelancers or jobseekers.', 'Henrik Thomsen', '¬∑ 2nd', 'Data Engineer', 'Sian is a top recruiter, she is very professinal and effective.<!----><br><!---->She goes a long way to help her clients find the right match.<!----><br><!---->Her dedication and commitment rubs off on people around her and paves the way for closing a deal or iron out any gaps that may have emerged between consultant and client.<!----><br><!---->I have been in the industry for many years, and I recognize rare talent when I see it!', 'Pagan Donaldson', '¬∑ 2nd', 'Talent Acquisition Manager | Hiring Exceptional Talent into our Business', \"Pagan was amazing throughout my whole application process with Evolution. She did everything she could to make me feel at ease during that time and she ensured I had all the information I needed to make my decision. I couldn't thank her enough for all her help and for putting up with my endless amount of questions!\", 'Honors &amp; awards', 'Girls Juvenile Captain', 'Issued by Donaghadee Golf Club ¬∑ Apr 2015', 'Languages', 'French', 'Spanish', 'Interests', 'Companies', 'Groups', 'Newsletters', 'Schools', 'Robert Half', 'BBC', 'Tech Freelance Support Network Copenhagen', 'Women in Machine Learning and Data Science', 'Data, People, &amp; Insurance', 'Bringing you weekly essentials on how data and talent are shaping the insurance industry', 'Andy Davis', 'Operationalizing DP&amp;P', 'A weekly lesson related to the operational and managerial side of data protection and privacy', 'Aaron Mendelsohn', 'University of Liverpool', 'Wolverhampton Grammar School', 'Other similar profiles', 'Sean Thompson', '¬∑ 2nd', 'Supporting companies in Denmark with high-quality UX/UI and Product consultants - Evolution Nordics - Denmark  üá©üá∞ - Podcast hostüéôÔ∏è', 'Jacob Byerley', '¬∑ 2nd', 'Recruitment Consultant at Evolution - Team Sweden.', 'Abi Stokes', '¬∑ 2nd', 'Security Freelance Recruitment Consultant at Evolution Recruitment Solutions | Nordics Team | Tech Podcast Host', 'Heather Nicholls', '¬∑ 2nd', 'üéÆ Gaming Recruitment Consultant | Podcast Host', 'Melanie Lindsey', '¬∑ 2nd', 'üéÆ I recruit Production, Marketing and Community consultants for Gaming Studios// Gaming Recruiter &amp; Podcast Host', 'People you may know', \"From Sian's company\", 'Gemma Thomason', 'üêßNordics Backend Recruitment Consultant | Podcast Host - We get your projects delivered through the provision of Tech Freelancers', 'Sophie Gould', 'üêß Nordics Divisional Director  ‚ú¥ Helping the delivery of IT projects through the provision of Tech Freelancers ‚ú¥', 'Jacob Byerley', 'Recruitment Consultant at Evolution - Team Sweden.', 'Sean Thompson', 'Supporting companies in Denmark with high-quality UX/UI and Product consultants - Evolution Nordics - Denmark  üá©üá∞ - Podcast hostüéôÔ∏è', 'Connor Leyland', 'Freelance Talent Provider (Full-Stack Development) for Danish Companies<span class=\"white-space-pre\"> </span><br><!---->&amp; Podcast Host- Evolution Exchange Podcast', 'You might like', 'Pages for you', 'Uneeq Interns', 'Education', 'TECH HIRING', 'IT Services and IT Consulting']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Read the HTML content from the file\n",
    "with open('html4.txt', 'r', encoding='utf-8') as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# Define regular expressions to extract the required information\n",
    "position_regex = re.compile(r'<div[^>]*class=\"text-body-medium break-words\"[^>]*>(.*?)</div>', re.DOTALL)\n",
    "time_period_regex = re.compile(r'<span aria-hidden=\"true\"><!---->(.*?)<!----></span>', re.DOTALL)\n",
    "about_regex = re.compile(r'<div class=\"YSRqXsgMMfWaTWfUvxOnnVEfdrxtuYUWyzOXA.*?inline-show-more-text--is-collapsed[^>]*>(.*?)</span>', re.DOTALL)\n",
    "skills_regex = re.compile(r'<span aria-hidden=\"true\"><!---->(.*?)<!----></span>', re.DOTALL)\n",
    "\n",
    "# Extract the position\n",
    "position_match = position_regex.search(html_content)\n",
    "position = position_match.group(1).strip() if position_match else 'Not found'\n",
    "\n",
    "# Extract the time period\n",
    "time_period_match = time_period_regex.search(html_content)\n",
    "time_period = time_period_match.group(1).strip() if time_period_match else 'Not found'\n",
    "\n",
    "# Extract the about section\n",
    "about_match = about_regex.search(html_content)\n",
    "about = re.sub(r'<.*?>', '', about_match.group(1)).strip() if about_match else 'Not found'\n",
    "\n",
    "# Extract the skills\n",
    "skills_matches = skills_regex.findall(html_content)\n",
    "skills = [skill.strip() for skill in skills_matches if skill.strip()]\n",
    "\n",
    "# Output the extracted information\n",
    "print(\"Position:\", position)\n",
    "print(\"Time Period:\", time_period)\n",
    "print(\"About:\", about)\n",
    "print(\"Skills:\", skills)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position: Data Recruitment Consultant (Nordicsüá©üá∞üêø) at Evolution Recruitment Solutions - We get your projects delivered through the provision of Tech Freelancers | Evolution Exchange Podcast Host\n",
      "Time Period: Highlights\n",
      "About: Evolution Nordics\n",
      "Skills: ['Highlights', 'Sian viewed your profile in the past 90 days', 'Reach out to Sian to understand their buying needs.', 'Free insight from Sales Navigator', 'About', 'I connect exciting Nordics tech companies with world-leading freelance talent. Underpinning my talent acquisition strategy are three main pillars:<!----><br><br><!---->Contribution ‚Äì I speak to Tech Leaders every day about the challenges they face and the insights they‚Äôve gained from the market ‚Äì insights that, up until now, were left unshared. Through my podcast platform, I‚Äôm sitting down with business leaders and sharing these insights with my network of freelancers. Check out my featured section for a sample of my latest content.<!----><br><span class=\"white-space-pre\"> </span><!----><!----><br><!---->Community ‚Äì I‚Äôm proud to host the Evolution Exchange Podcast Nordics ‚Äì one of the Nordics fastest growing tech podcasts. With well over 250 daily listeners, the Evolution Exchange Podcast Nordics is a hive of discussion where the best companies and freelancers come together to exchange tips, tricks and advice.<!----><br><br><!---->Collaboration ‚Äì I work with an incredible team of talented and ambitious recruiters who are specialists in sourcing people for agile development teams.<span class=\"white-space-pre\"> </span><br><br><!---->If you would like to contribute your insights with our growing community of Copenhagen based freelancers, or are looking for your next assignment, click connect.', 'Featured', 'Document', 'Our recruitment services', 'A more in-depth overview of the services we provide for our clients in Denmark', 'Post', 'Link', 'Evo Nordics #322 - How To Freelance In Denmark', 'Spotify', 'Activity', '2,272 followers', 'Experience', 'Evolution Nordics', '2 yrs', 'Senior Recruitment Consultant', 'Full-time', 'Warrington, England, United Kingdom', 'Consistent Recruitment Consultant<span class=\"white-space-pre\"> </span></span><span class=\"visually-hidden\"><!---->Consistent Recruitment Consultant<span class=\"white-space-pre\"> </span></span>\\n          </div>\\n      \\n      </div>\\n  \\n    </div>\\n  \\n<!----><!----><!---->            </div>\\n<!---->              <span class=\"t-14 t-normal\\n                  t-black--light\">\\n                <span class=\"pvs-entity__caption-wrapper\" aria-hidden=\"true\"><!---->Aug 2023 - Dec 2023 ¬∑ 5 mos', 'Recruitment Consultant', 'Auxiliare de conversaci√≥n', 'British Council ¬∑ Full-time', 'Murcia, Regi√≥n de Murcia, Spain', 'Customer Service Assistant', 'Iceland Foods ¬∑ Part-time', 'Liverpool, England, United Kingdom', 'Bartender', 'Liverpool Football Club ¬∑ Part-time', 'Liverpool, England, United Kingdom', 'Customer Service Staff', 'Mauds Ice Creams Ltd', 'Newtownards, County Down, Northern Ireland', 'Education', 'University of Liverpool', 'Geography BSc, Geography', 'Grade: 1st class honours', 'Activities and societies: Member of the hockey team and geography society. I also play netball in the campus league.', 'Regent House Grammar School', 'Grade: 10 GCSEs (A*-C) 3 A-levels (BCC)', 'Activities and societies: Hockey team, golf team, school council, learning mentor, year 8 mentor, prefect.', 'A* in maths and A in English at GCSE', 'Licenses &amp; certifications', 'Teaching English as a Foreign Language (TEFL)', 'University of Cambridge', 'Driver', 'Driver and Vehicle Licensing Agency (DVLA)', 'Volunteering', 'Volunteer group in school', 'Habitat for Humanity', 'Poverty Alleviation', 'A group of students in my grammar school and a group from another came together to learn about this great organisation. Together we improved a church in our community by working as a team to clean up, paint and plaster. We also had a task of raising ¬£1,000 which we did through different things within school such as a paid non uniform day, a talent show and a bun sale.', 'Assistant', 'Donaghadee Primary School', 'Education', 'A week long work experience as a classroom assistant with Year 2s (5/6 year olds). Marking work, teaching them a couple of things from A level languages, helping the more challenged children and staying behind to help clean up and help wherever needed throughout the school. I was also put on playground duty at break and lunch.', 'Skills', 'Geographic Information Systems (GIS)', 'Endorsed by 1 people in the last 6 months', '4 endorsements', 'Customer Service', 'Endorsed by 1 people in the last 6 months', '3 endorsements', 'Recommendations', 'Received', 'Given', 'Troy (Kyung Un) Choi', '¬∑ 2nd', 'Freelance Cloud Data Solution &amp; Architecture', 'I highly recommend Sian Vance as a recruitment consultant for anyone seeking new opportunities in the tech industry. Sian helped me secure a new project at TDC Net and I had a wonderful experience throughout the process thanks to her expert advice and support. Her knowledge of the industry, attention to detail, and excellent communication skills make her an invaluable asset to anyone looking for a new role. I would not hesitate to recommend Sian to other freelancers or jobseekers.', 'Henrik Thomsen', '¬∑ 2nd', 'Data Engineer', 'Sian is a top recruiter, she is very professinal and effective.<!----><br><!---->She goes a long way to help her clients find the right match.<!----><br><!---->Her dedication and commitment rubs off on people around her and paves the way for closing a deal or iron out any gaps that may have emerged between consultant and client.<!----><br><!---->I have been in the industry for many years, and I recognize rare talent when I see it!', 'Pagan Donaldson', '¬∑ 2nd', 'Talent Acquisition Manager | Hiring Exceptional Talent into our Business', \"Pagan was amazing throughout my whole application process with Evolution. She did everything she could to make me feel at ease during that time and she ensured I had all the information I needed to make my decision. I couldn't thank her enough for all her help and for putting up with my endless amount of questions!\", 'Honors &amp; awards', 'Girls Juvenile Captain', 'Issued by Donaghadee Golf Club ¬∑ Apr 2015', 'Languages', 'French', 'Spanish', 'Interests', 'Companies', 'Groups', 'Newsletters', 'Schools', 'Robert Half', 'BBC', 'Tech Freelance Support Network Copenhagen', 'Women in Machine Learning and Data Science', 'Data, People, &amp; Insurance', 'Bringing you weekly essentials on how data and talent are shaping the insurance industry', 'Andy Davis', 'Operationalizing DP&amp;P', 'A weekly lesson related to the operational and managerial side of data protection and privacy', 'Aaron Mendelsohn', 'University of Liverpool', 'Wolverhampton Grammar School', 'Other similar profiles', 'Sean Thompson', '¬∑ 2nd', 'Supporting companies in Denmark with high-quality UX/UI and Product consultants - Evolution Nordics - Denmark  üá©üá∞ - Podcast hostüéôÔ∏è', 'Jacob Byerley', '¬∑ 2nd', 'Recruitment Consultant at Evolution - Team Sweden.', 'Abi Stokes', '¬∑ 2nd', 'Security Freelance Recruitment Consultant at Evolution Recruitment Solutions | Nordics Team | Tech Podcast Host', 'Heather Nicholls', '¬∑ 2nd', 'üéÆ Gaming Recruitment Consultant | Podcast Host', 'Melanie Lindsey', '¬∑ 2nd', 'üéÆ I recruit Production, Marketing and Community consultants for Gaming Studios// Gaming Recruiter &amp; Podcast Host', 'People you may know', \"From Sian's company\", 'Gemma Thomason', 'üêßNordics Backend Recruitment Consultant | Podcast Host - We get your projects delivered through the provision of Tech Freelancers', 'Sophie Gould', 'üêß Nordics Divisional Director  ‚ú¥ Helping the delivery of IT projects through the provision of Tech Freelancers ‚ú¥', 'Jacob Byerley', 'Recruitment Consultant at Evolution - Team Sweden.', 'Sean Thompson', 'Supporting companies in Denmark with high-quality UX/UI and Product consultants - Evolution Nordics - Denmark  üá©üá∞ - Podcast hostüéôÔ∏è', 'Connor Leyland', 'Freelance Talent Provider (Full-Stack Development) for Danish Companies<span class=\"white-space-pre\"> </span><br><!---->&amp; Podcast Host- Evolution Exchange Podcast', 'You might like', 'Pages for you', 'Uneeq Interns', 'Education', 'TECH HIRING', 'IT Services and IT Consulting']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def read_html_file(file_path):\n",
    "    \"\"\"Read the HTML content from the file.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "def extract_position(html_content):\n",
    "    \"\"\"Extract the position from the HTML content.\"\"\"\n",
    "    position_regex = re.compile(r'<div[^>]*class=\"text-body-medium break-words\"[^>]*>(.*?)</div>', re.DOTALL)\n",
    "    position_match = position_regex.search(html_content)\n",
    "    return position_match.group(1).strip() if position_match else 'Not found'\n",
    "\n",
    "def extract_time_period(html_content):\n",
    "    \"\"\"Extract the time period from the HTML content.\"\"\"\n",
    "    time_period_regex = re.compile(r'<span aria-hidden=\"true\"><!---->(.*?)<!----></span>', re.DOTALL)\n",
    "    time_period_match = time_period_regex.search(html_content)\n",
    "    return time_period_match.group(1).strip() if time_period_match else 'Not found'\n",
    "\n",
    "def extract_about(html_content):\n",
    "    \"\"\"Extract the about section from the HTML content.\"\"\"\n",
    "    about_regex = re.compile(r'<div class=\"YSRqXsgMMfWaTWfUvxOnnVEfdrxtuYUWyzOXA.*?inline-show-more-text--is-collapsed[^>]*>(.*?)</span>', re.DOTALL)\n",
    "    about_match = about_regex.search(html_content)\n",
    "    return re.sub(r'<.*?>', '', about_match.group(1)).strip() if about_match else 'Not found'\n",
    "\n",
    "def extract_skills(html_content):\n",
    "    \"\"\"Extract the skills from the HTML content.\"\"\"\n",
    "    skills_regex = re.compile(r'<span aria-hidden=\"true\"><!---->(.*?)<!----></span>', re.DOTALL)\n",
    "    skills_matches = skills_regex.findall(html_content)\n",
    "    return [skill.strip() for skill in skills_matches if skill.strip()]\n",
    "\n",
    "def main():\n",
    "    # Read the HTML content from the file\n",
    "    html_content = read_html_file('html4.txt')\n",
    "\n",
    "    # Extract the required information\n",
    "    position = extract_position(html_content)\n",
    "    time_period = extract_time_period(html_content)\n",
    "    about = extract_about(html_content)\n",
    "    skills = extract_skills(html_content)\n",
    "\n",
    "    # Output the extracted information\n",
    "    print(\"Position:\", position)\n",
    "    print(\"Time Period:\", time_period)\n",
    "    print(\"About:\", about)\n",
    "    print(\"Skills:\", skills)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highlights:\n",
      "Current Position: Data Recruitment Consultant (Nordicsüá©üá∞üêø) at Evolution Recruitment Solutions - We get your projects delivered through the provision of Tech Freelancers | Evolution Exchange Podcast Host\n",
      "\n",
      "About:\n",
      "Evolution Nordics\n",
      "\n",
      "Experience:\n",
      "\n",
      "Responsibilities:\n",
      "- Not found\n",
      "\n",
      "Skills:\n",
      "- Sian viewed your profile in the past 90 days\n",
      "- Reach out to Sian to understand their buying needs.\n",
      "- Free insight from Sales Navigator\n",
      "- I connect exciting Nordics tech companies with world-leading freelance talent. Underpinning my talent acquisition strategy are three main pillars:<!----><br><br><!---->Contribution ‚Äì I speak to Tech Leaders every day about the challenges they face and the insights they‚Äôve gained from the market ‚Äì insights that, up until now, were left unshared. Through my podcast platform, I‚Äôm sitting down with business leaders and sharing these insights with my network of freelancers. Check out my featured section for a sample of my latest content.<!----><br><span class=\"white-space-pre\"> </span><!----><!----><br><!---->Community ‚Äì I‚Äôm proud to host the Evolution Exchange Podcast Nordics ‚Äì one of the Nordics fastest growing tech podcasts. With well over 250 daily listeners, the Evolution Exchange Podcast Nordics is a hive of discussion where the best companies and freelancers come together to exchange tips, tricks and advice.<!----><br><br><!---->Collaboration ‚Äì I work with an incredible team of talented and ambitious recruiters who are specialists in sourcing people for agile development teams.<span class=\"white-space-pre\"> </span><br><br><!---->If you would like to contribute your insights with our growing community of Copenhagen based freelancers, or are looking for your next assignment, click connect.\n",
      "- Featured\n",
      "- Document\n",
      "- Our recruitment services\n",
      "- A more in-depth overview of the services we provide for our clients in Denmark\n",
      "- Post\n",
      "- Link\n",
      "- Evo Nordics #322 - How To Freelance In Denmark\n",
      "- Spotify\n",
      "- 2,272 followers\n",
      "- Evolution Nordics\n",
      "- 2 yrs\n",
      "- Senior Recruitment Consultant\n",
      "- Full-time\n",
      "- Warrington, England, United Kingdom\n",
      "- Consistent Recruitment Consultant<span class=\"white-space-pre\"> </span></span><span class=\"visually-hidden\"><!---->Consistent Recruitment Consultant<span class=\"white-space-pre\"> </span></span>\n",
      "          </div>\n",
      "      \n",
      "      </div>\n",
      "  \n",
      "    </div>\n",
      "  \n",
      "<!----><!----><!---->            </div>\n",
      "<!---->              <span class=\"t-14 t-normal\n",
      "                  t-black--light\">\n",
      "                <span class=\"pvs-entity__caption-wrapper\" aria-hidden=\"true\"><!---->Aug 2023 - Dec 2023 ¬∑ 5 mos\n",
      "- Recruitment Consultant\n",
      "- Auxiliare de conversaci√≥n\n",
      "- British Council ¬∑ Full-time\n",
      "- Murcia, Regi√≥n de Murcia, Spain\n",
      "- Customer Service Assistant\n",
      "- Iceland Foods ¬∑ Part-time\n",
      "- Liverpool, England, United Kingdom\n",
      "- Bartender\n",
      "- Liverpool Football Club ¬∑ Part-time\n",
      "- Liverpool, England, United Kingdom\n",
      "- Customer Service Staff\n",
      "- Mauds Ice Creams Ltd\n",
      "- Newtownards, County Down, Northern Ireland\n",
      "- Education\n",
      "- University of Liverpool\n",
      "- Geography BSc, Geography\n",
      "- Grade: 1st class honours\n",
      "- Activities and societies: Member of the hockey team and geography society. I also play netball in the campus league.\n",
      "- Regent House Grammar School\n",
      "- Grade: 10 GCSEs (A*-C) 3 A-levels (BCC)\n",
      "- Activities and societies: Hockey team, golf team, school council, learning mentor, year 8 mentor, prefect.\n",
      "- A* in maths and A in English at GCSE\n",
      "- Licenses &amp; certifications\n",
      "- Teaching English as a Foreign Language (TEFL)\n",
      "- University of Cambridge\n",
      "- Driver\n",
      "- Driver and Vehicle Licensing Agency (DVLA)\n",
      "- Volunteering\n",
      "- Volunteer group in school\n",
      "- Habitat for Humanity\n",
      "- Poverty Alleviation\n",
      "- A group of students in my grammar school and a group from another came together to learn about this great organisation. Together we improved a church in our community by working as a team to clean up, paint and plaster. We also had a task of raising ¬£1,000 which we did through different things within school such as a paid non uniform day, a talent show and a bun sale.\n",
      "- Assistant\n",
      "- Donaghadee Primary School\n",
      "- Education\n",
      "- A week long work experience as a classroom assistant with Year 2s (5/6 year olds). Marking work, teaching them a couple of things from A level languages, helping the more challenged children and staying behind to help clean up and help wherever needed throughout the school. I was also put on playground duty at break and lunch.\n",
      "- Geographic Information Systems (GIS)\n",
      "- Endorsed by 1 people in the last 6 months\n",
      "- 4 endorsements\n",
      "- Customer Service\n",
      "- Endorsed by 1 people in the last 6 months\n",
      "- 3 endorsements\n",
      "- Recommendations\n",
      "- Received\n",
      "- Given\n",
      "- Troy (Kyung Un) Choi\n",
      "- ¬∑ 2nd\n",
      "- Freelance Cloud Data Solution &amp; Architecture\n",
      "- I highly recommend Sian Vance as a recruitment consultant for anyone seeking new opportunities in the tech industry. Sian helped me secure a new project at TDC Net and I had a wonderful experience throughout the process thanks to her expert advice and support. Her knowledge of the industry, attention to detail, and excellent communication skills make her an invaluable asset to anyone looking for a new role. I would not hesitate to recommend Sian to other freelancers or jobseekers.\n",
      "- Henrik Thomsen\n",
      "- ¬∑ 2nd\n",
      "- Data Engineer\n",
      "- Sian is a top recruiter, she is very professinal and effective.<!----><br><!---->She goes a long way to help her clients find the right match.<!----><br><!---->Her dedication and commitment rubs off on people around her and paves the way for closing a deal or iron out any gaps that may have emerged between consultant and client.<!----><br><!---->I have been in the industry for many years, and I recognize rare talent when I see it!\n",
      "- Pagan Donaldson\n",
      "- ¬∑ 2nd\n",
      "- Talent Acquisition Manager | Hiring Exceptional Talent into our Business\n",
      "- Pagan was amazing throughout my whole application process with Evolution. She did everything she could to make me feel at ease during that time and she ensured I had all the information I needed to make my decision. I couldn't thank her enough for all her help and for putting up with my endless amount of questions!\n",
      "- Honors &amp; awards\n",
      "- Girls Juvenile Captain\n",
      "- Issued by Donaghadee Golf Club ¬∑ Apr 2015\n",
      "- Languages\n",
      "- French\n",
      "- Spanish\n",
      "- Interests\n",
      "- Companies\n",
      "- Groups\n",
      "- Newsletters\n",
      "- Schools\n",
      "- Robert Half\n",
      "- BBC\n",
      "- Tech Freelance Support Network Copenhagen\n",
      "- Women in Machine Learning and Data Science\n",
      "- Data, People, &amp; Insurance\n",
      "- Bringing you weekly essentials on how data and talent are shaping the insurance industry\n",
      "- Andy Davis\n",
      "- Operationalizing DP&amp;P\n",
      "- A weekly lesson related to the operational and managerial side of data protection and privacy\n",
      "- Aaron Mendelsohn\n",
      "- University of Liverpool\n",
      "- Wolverhampton Grammar School\n",
      "- Other similar profiles\n",
      "- Sean Thompson\n",
      "- ¬∑ 2nd\n",
      "- Supporting companies in Denmark with high-quality UX/UI and Product consultants - Evolution Nordics - Denmark  üá©üá∞ - Podcast hostüéôÔ∏è\n",
      "- Jacob Byerley\n",
      "- ¬∑ 2nd\n",
      "- Recruitment Consultant at Evolution - Team Sweden.\n",
      "- Abi Stokes\n",
      "- ¬∑ 2nd\n",
      "- Security Freelance Recruitment Consultant at Evolution Recruitment Solutions | Nordics Team | Tech Podcast Host\n",
      "- Heather Nicholls\n",
      "- ¬∑ 2nd\n",
      "- üéÆ Gaming Recruitment Consultant | Podcast Host\n",
      "- Melanie Lindsey\n",
      "- ¬∑ 2nd\n",
      "- üéÆ I recruit Production, Marketing and Community consultants for Gaming Studios// Gaming Recruiter &amp; Podcast Host\n",
      "- People you may know\n",
      "- From Sian's company\n",
      "- Gemma Thomason\n",
      "- üêßNordics Backend Recruitment Consultant | Podcast Host - We get your projects delivered through the provision of Tech Freelancers\n",
      "- Sophie Gould\n",
      "- üêß Nordics Divisional Director  ‚ú¥ Helping the delivery of IT projects through the provision of Tech Freelancers ‚ú¥\n",
      "- Jacob Byerley\n",
      "- Recruitment Consultant at Evolution - Team Sweden.\n",
      "- Sean Thompson\n",
      "- Supporting companies in Denmark with high-quality UX/UI and Product consultants - Evolution Nordics - Denmark  üá©üá∞ - Podcast hostüéôÔ∏è\n",
      "- Connor Leyland\n",
      "- Freelance Talent Provider (Full-Stack Development) for Danish Companies<span class=\"white-space-pre\"> </span><br><!---->&amp; Podcast Host- Evolution Exchange Podcast\n",
      "- You might like\n",
      "- Pages for you\n",
      "- Uneeq Interns\n",
      "- Education\n",
      "- TECH HIRING\n",
      "- IT Services and IT Consulting\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Read the HTML content from the file\n",
    "with open('html4.txt', 'r', encoding='utf-8') as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# Define regular expressions to extract the required information\n",
    "position_regex = re.compile(r'<div[^>]*class=\"text-body-medium break-words\"[^>]*>(.*?)</div>', re.DOTALL)\n",
    "about_regex = re.compile(r'<div class=\"YSRqXsgMMfWaTWfUvxOnnVEfdrxtuYUWyzOXA.*?inline-show-more-text--is-collapsed[^>]*>(.*?)</span>', re.DOTALL)\n",
    "experience_regex = re.compile(r'<span class=\"visually-hidden\"><!---->([^<]+)</span>\\s*</div>\\s*</div>\\s*<span class=\"t-14 t-normal t-black--light\">\\s*<span class=\"pvs-entity__caption-wrapper\" aria-hidden=\"true\"><!---->([^<]+)', re.DOTALL)\n",
    "responsibilities_regex = re.compile(r'Responsibilities :-<!----><br><!---->(.+?)(?=<br><br>|$)', re.DOTALL)\n",
    "skills_regex = re.compile(r'<span aria-hidden=\"true\"><!---->(.*?)<!----></span>', re.DOTALL)\n",
    "\n",
    "# Extract the information\n",
    "position_match = position_regex.search(html_content)\n",
    "position = position_match.group(1).strip() if position_match else 'Not found'\n",
    "\n",
    "about_match = about_regex.search(html_content)\n",
    "about = re.sub(r'<.*?>', '', about_match.group(1)).strip() if about_match else 'Not found'\n",
    "\n",
    "experience_matches = experience_regex.findall(html_content)\n",
    "experiences = [f\"{title.strip()} - {company.strip()}\" for title, company in experience_matches]\n",
    "\n",
    "responsibilities_match = responsibilities_regex.search(html_content)\n",
    "responsibilities = responsibilities_match.group(1).strip().split('<br><!---->') if responsibilities_match else ['Not found']\n",
    "\n",
    "skills_matches = skills_regex.findall(html_content)\n",
    "skills = [skill.strip() for skill in skills_matches if skill.strip() and skill.strip() not in ['Highlights', 'About', 'Activity', 'Experience', 'Skills']]\n",
    "\n",
    "# Output the extracted information in the requested format\n",
    "print(\"Highlights:\")\n",
    "print(f\"Current Position: {position}\\n\")\n",
    "\n",
    "print(\"About:\")\n",
    "print(f\"{about}\\n\")\n",
    "\n",
    "print(\"Experience:\")\n",
    "for exp in experiences:\n",
    "    print(f\"- {exp}\")\n",
    "print()\n",
    "\n",
    "print(\"Responsibilities:\")\n",
    "for resp in responsibilities:\n",
    "    print(f\"- {resp.strip()}\")\n",
    "print()\n",
    "\n",
    "print(\"Skills:\")\n",
    "for skill in skills:\n",
    "    print(f\"- {skill}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2197"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2197"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I connect exciting Nordics tech companies with world-leading freelance talent. Underpinning my talent acquisition strategy are three main pillars:Contribution ‚Äì I speak to Tech Leaders every day about the challenges they face and the insights they‚Äôve gained from the market ‚Äì insights that, up until now, were left unshared. Through my podcast platform, I‚Äôm sitting down with business leaders and sharing these insights with my network of freelancers. Check out my featured section for a sample of my latest content.Community ‚Äì I‚Äôm proud to host the Evolution Exchange Podcast Nordics ‚Äì one of the Nordics fastest growing tech podcasts. With well over 250 daily listeners, the Evolution Exchange Podcast Nordics is a hive of discussion where the best companies and freelancers come together to exchange tips, tricks and advice.Collaboration ‚Äì I work with an incredible team of talented and ambitious recruiters who are specialists in sourcing people for agile development teams.If you would like to contribute your insights with our growing community of Copenhagen based freelancers, or are looking for your next assignment, click connect.I connect exciting Nordics tech companies with world-leading freelance talent. Underpinning my talent acquisition strategy are three main pillars:\n",
      "\n",
      "Contribution ‚Äì I speak to Tech Leaders every day about the challenges they face and the insights they‚Äôve gained from the market ‚Äì insights that, up until now, were left unshared. Through my podcast platform, I‚Äôm sitting down with business leaders and sharing these insights with my network of freelancers. Check out my featured section for a sample of my latest content.\n",
      " \n",
      "Community ‚Äì I‚Äôm proud to host the Evolution Exchange Podcast Nordics ‚Äì one of the Nordics fastest growing tech podcasts. With well over 250 daily listeners, the Evolution Exchange Podcast Nordics is a hive of discussion where the best companies and freelancers come together to exchange tips, tricks and advice.\n",
      "\n",
      "Collaboration ‚Äì I work with an incredible team of talented and ambitious recruiters who are specialists in sourcing people for agile development teams. \n",
      "\n",
      "If you would like to contribute your insights with our growing community of Copenhagen based freelancers, or are looking for your next assignment, click connect.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Read the HTML file\n",
    "with open('html4.txt', 'r', encoding='utf-8') as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Find the \"About\" section more specifically\n",
    "about_header = soup.find('div', {'id': 'about'})\n",
    "\n",
    "# Ensure we find the specific parent container that contains the \"About\" text\n",
    "if about_header:\n",
    "    about_section = about_header.find_next('div', class_='display-flex ph5 pv3')\n",
    "\n",
    "    if about_section:\n",
    "        # Extract the text content from each child element separately\n",
    "        about_text = \"\"\n",
    "        for child in about_section.children:\n",
    "            if child.name is not None:  # Exclude NavigableString objects\n",
    "                text = child.get_text(strip=True)\n",
    "                if text and text not in about_text:  # Avoid duplicates\n",
    "                    about_text += text + \"\\n\"\n",
    "\n",
    "        # Print the extracted \"About\" section\n",
    "        print(about_text.strip())\n",
    "    else:\n",
    "        print(\"About section content not found\")\n",
    "else:\n",
    "    print(\"About section not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'company': '<!---->Evolution Nordics<!---->', 'position': '<!---->Evolution Nordics<!---->', 'duration': '<!---->2 yrs<!---->', 'location': '<!---->Full-time<!---->'}\n",
      "{'company': '<!---->Auxiliare de conversaci√≥n<!---->', 'position': '<!---->Auxiliare de conversaci√≥n<!---->', 'duration': '<!---->British Council ¬∑ Full-time<!---->', 'location': '<!---->Murcia, Regi√≥n de Murcia, Spain<!---->'}\n",
      "{'company': '<!---->Customer Service Assistant<!---->', 'position': '<!---->Customer Service Assistant<!---->', 'duration': '<!---->Iceland Foods ¬∑ Part-time<!---->', 'location': '<!---->Liverpool, England, United Kingdom<!---->'}\n",
      "{'company': '<!---->Bartender<!---->', 'position': '<!---->Bartender<!---->', 'duration': '<!---->Liverpool Football Club ¬∑ Part-time<!---->', 'location': '<!---->Liverpool, England, United Kingdom<!---->'}\n",
      "{'company': '<!---->Customer Service Staff<!---->', 'position': '<!---->Customer Service Staff<!---->', 'duration': '<!---->Mauds Ice Creams Ltd<!---->', 'location': '<!---->Newtownards, County Down, Northern Ireland<!---->'}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "with open('html4.txt', 'r', encoding='utf-8') as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# Regular expression to extract experience section\n",
    "experience_pattern = re.compile(r'<div id=\"experience\".*?</section>', re.DOTALL)\n",
    "experience_section = experience_pattern.search(html_content)\n",
    "\n",
    "if experience_section:\n",
    "    experience_html = experience_section.group()\n",
    "    \n",
    "    # Regular expression to extract individual experience items\n",
    "    item_pattern = re.compile(r'<li class=\"artdeco-list__item.*?</li>', re.DOTALL)\n",
    "    items = item_pattern.findall(experience_html)\n",
    "\n",
    "    experiences = []\n",
    "    for item in items:\n",
    "        experience = {}\n",
    "        \n",
    "        # Extract company\n",
    "        company_match = re.search(r'data-field=\"experience_company_logo\".*?<span aria-hidden=\"true\">(.*?)</span>', item, re.DOTALL)\n",
    "        if company_match:\n",
    "            experience['company'] = company_match.group(1).strip()\n",
    "        \n",
    "        # Extract position\n",
    "        position_match = re.search(r'<span aria-hidden=\"true\">(.*?)</span>', item)\n",
    "        if position_match:\n",
    "            experience['position'] = position_match.group(1).strip()\n",
    "        \n",
    "        # Extract duration and location\n",
    "        duration_match = re.findall(r'<span class=\"t-14 t-normal.*?<span aria-hidden=\"true\">(.*?)</span>', item, re.DOTALL)\n",
    "        if duration_match:\n",
    "            experience['duration'] = duration_match[0].strip()\n",
    "            experience['location'] = duration_match[1].strip() if len(duration_match) > 1 else ''\n",
    "\n",
    "        experiences.append(experience)\n",
    "    \n",
    "    # Print the extracted experiences\n",
    "    for exp in experiences:\n",
    "        print(exp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'company': '<!---->Evolution Nordics<!---->', 'duration': '<!---->Dec 2023 - Present ¬∑ 8 mos<!---->', 'location': '<!---->Warrington, England, United Kingdom<!---->'}\n",
      "{'company': '<!---->Auxiliare de conversaci√≥n<!---->', 'duration': '<!---->Oct 2021 - Aug 2022 ¬∑ 11 mos<!---->', 'location': ''}\n",
      "{'company': '<!---->Customer Service Assistant<!---->', 'duration': '<!---->Oct 2020 - Oct 2021 ¬∑ 1 yr 1 mo<!---->', 'location': ''}\n",
      "{'company': '<!---->Bartender<!---->', 'duration': '<!---->Mar 2019 - Oct 2021 ¬∑ 2 yrs 8 mos<!---->', 'location': ''}\n",
      "{'company': '<!---->Customer Service Staff<!---->', 'duration': '<!---->Feb 2018 - Oct 2020 ¬∑ 2 yrs 9 mos<!---->', 'location': ''}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "with open('html4.txt', 'r', encoding='utf-8') as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# Regular expression to extract experience section\n",
    "experience_pattern = re.compile(r'<div id=\"experience\".*?</section>', re.DOTALL)\n",
    "experience_section = experience_pattern.search(html_content)\n",
    "\n",
    "if experience_section:\n",
    "    experience_html = experience_section.group()\n",
    "    \n",
    "    # Regular expression to extract individual experience items\n",
    "    item_pattern = re.compile(r'<li class=\"artdeco-list__item.*?</li>', re.DOTALL)\n",
    "    items = item_pattern.findall(experience_html)\n",
    "\n",
    "    experiences = []\n",
    "    for item in items:\n",
    "        experience = {}\n",
    "        \n",
    "        # Extract company\n",
    "        company_match = re.search(r'data-field=\"experience_company_logo\".*?<span aria-hidden=\"true\">(.*?)</span>', item, re.DOTALL)\n",
    "        if company_match:\n",
    "            experience['company'] = company_match.group(1).strip()\n",
    "        \n",
    "        # Extract position\n",
    "        position_match = re.search(r'<div class=\"display-flex align-items-center mr1 hoverable-link-text t-bold\">.*?<span aria-hidden=\"true\">(.*?)</span>', item, re.DOTALL)\n",
    "        if position_match:\n",
    "            experience['position'] = position_match.group(1).strip()\n",
    "        \n",
    "        # Extract duration\n",
    "        duration_match = re.search(r'<span class=\"pvs-entity__caption-wrapper\" aria-hidden=\"true\">(.*?)</span>', item)\n",
    "        if duration_match:\n",
    "            experience['duration'] = duration_match.group(1).strip()\n",
    "        \n",
    "        # Extract location\n",
    "        location_match = re.search(r'<span aria-hidden=\"true\">(.*?)</span>\\s*<span class=\"visually-hidden\">.*?</span>\\s*</span>\\s*</a>', item)\n",
    "        if location_match:\n",
    "            experience['location'] = location_match.group(1).strip()\n",
    "        else:\n",
    "            experience['location'] = ''\n",
    "\n",
    "        experiences.append(experience)\n",
    "    \n",
    "    # Print the extracted experiences\n",
    "    for exp in experiences:\n",
    "        print(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'company': 'Evolution Nordics', 'duration': 'Dec 2023 - Present ¬∑ 8 mos', 'location': 'Warrington, England, United Kingdom'}\n",
      "{'company': 'Auxiliare de conversaci√≥n', 'duration': 'Oct 2021 - Aug 2022 ¬∑ 11 mos', 'location': 'Murcia, Regi√≥n de Murcia, Spain'}\n",
      "{'company': 'Customer Service Assistant', 'duration': 'Oct 2020 - Oct 2021 ¬∑ 1 yr 1 mo', 'location': 'Liverpool, England, United Kingdom'}\n",
      "{'company': 'Bartender', 'duration': 'Mar 2019 - Oct 2021 ¬∑ 2 yrs 8 mos', 'location': 'Liverpool, England, United Kingdom'}\n",
      "{'company': 'Customer Service Staff', 'duration': 'Feb 2018 - Oct 2020 ¬∑ 2 yrs 9 mos', 'location': 'Newtownards, County Down, Northern Ireland'}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "with open('html4.txt', 'r', encoding='utf-8') as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# Regular expression to extract experience section\n",
    "experience_pattern = re.compile(r'<div id=\"experience\".*?</section>', re.DOTALL)\n",
    "experience_section = experience_pattern.search(html_content)\n",
    "\n",
    "if experience_section:\n",
    "    experience_html = experience_section.group()\n",
    "    \n",
    "    # Regular expression to extract individual experience items\n",
    "    item_pattern = re.compile(r'<li class=\"artdeco-list__item.*?</li>', re.DOTALL)\n",
    "    items = item_pattern.findall(experience_html)\n",
    "\n",
    "    experiences = []\n",
    "    for item in items:\n",
    "        experience = {}\n",
    "        \n",
    "        # Extract company\n",
    "        company_match = re.search(r'data-field=\"experience_company_logo\".*?<span aria-hidden=\"true\">(.*?)</span>', item, re.DOTALL)\n",
    "        if company_match:\n",
    "            experience['company'] = re.sub(r'<.*?>', '', company_match.group(1)).strip()\n",
    "        \n",
    "        # Extract position\n",
    "        position_match = re.search(r'<div class=\"display-flex align-items-center mr1 hoverable-link-text t-bold\">(.*?)</div>', item, re.DOTALL)\n",
    "        if position_match:\n",
    "            experience['position'] = re.sub(r'<.*?>', '', position_match.group(1)).strip()\n",
    "        \n",
    "        # Extract duration\n",
    "        duration_match = re.search(r'<span class=\"pvs-entity__caption-wrapper\" aria-hidden=\"true\">(.*?)</span>', item)\n",
    "        if duration_match:\n",
    "            experience['duration'] = re.sub(r'<.*?>', '', duration_match.group(1)).strip()\n",
    "        \n",
    "        # Extract location\n",
    "        location_match = re.findall(r'<span aria-hidden=\"true\">(.*?)</span>', item)\n",
    "        if len(location_match) > 1:\n",
    "            experience['location'] = re.sub(r'<.*?>', '', location_match[-1]).strip()\n",
    "        else:\n",
    "            experience['location'] = ''\n",
    "\n",
    "        experiences.append(experience)\n",
    "    \n",
    "    # Print the extracted experiences\n",
    "    for exp in experiences:\n",
    "        print(exp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'company': 'Evolution Nordics', 'position': '', 'duration': 'Dec 2023 - Present ¬∑ 8 mos', 'location': 'Warrington, England, United Kingdom'}\n",
      "{'company': 'Auxiliare de conversaci√≥n', 'position': '', 'duration': 'Oct 2021 - Aug 2022 ¬∑ 11 mos', 'location': 'Murcia, Regi√≥n de Murcia, Spain'}\n",
      "{'company': 'Customer Service Assistant', 'position': '', 'duration': 'Oct 2020 - Oct 2021 ¬∑ 1 yr 1 mo', 'location': 'Liverpool, England, United Kingdom'}\n",
      "{'company': 'Bartender', 'position': '', 'duration': 'Mar 2019 - Oct 2021 ¬∑ 2 yrs 8 mos', 'location': 'Liverpool, England, United Kingdom'}\n",
      "{'company': 'Customer Service Staff', 'position': '', 'duration': 'Feb 2018 - Oct 2020 ¬∑ 2 yrs 9 mos', 'location': 'Newtownards, County Down, Northern Ireland'}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "with open('html4.txt', 'r', encoding='utf-8') as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# Regular expression to extract experience section\n",
    "experience_pattern = re.compile(r'<div id=\"experience\".*?</section>', re.DOTALL)\n",
    "experience_section = experience_pattern.search(html_content)\n",
    "\n",
    "if experience_section:\n",
    "    experience_html = experience_section.group()\n",
    "    \n",
    "    # Regular expression to extract individual experience items\n",
    "    item_pattern = re.compile(r'<li class=\"artdeco-list__item.*?</li>', re.DOTALL)\n",
    "    items = item_pattern.findall(experience_html)\n",
    "\n",
    "    experiences = []\n",
    "    for item in items:\n",
    "        experience = {'company': '', 'position': '', 'duration': '', 'location': ''}\n",
    "        \n",
    "        # Extract company\n",
    "        company_match = re.search(r'data-field=\"experience_company_logo\".*?<span aria-hidden=\"true\">(.*?)</span>', item, re.DOTALL)\n",
    "        if company_match:\n",
    "            experience['company'] = re.sub(r'<.*?>', '', company_match.group(1)).strip()\n",
    "        \n",
    "        # Extract position\n",
    "        position_match = re.search(r'<div class=\"display-flex align-items-center mr1 hoverable-link-text t-bold\">(.*?)</div>', item, re.DOTALL)\n",
    "        if position_match:\n",
    "            experience['position'] = re.sub(r'<.*?>', '', position_match.group(1)).strip()\n",
    "        \n",
    "        # Extract duration\n",
    "        duration_match = re.search(r'<span class=\"pvs-entity__caption-wrapper\" aria-hidden=\"true\">(.*?)</span>', item)\n",
    "        if duration_match:\n",
    "            experience['duration'] = re.sub(r'<.*?>', '', duration_match.group(1)).strip()\n",
    "        \n",
    "        # Extract location\n",
    "        location_match = re.findall(r'<span aria-hidden=\"true\">(.*?)</span>', item)\n",
    "        if len(location_match) > 1:\n",
    "            experience['location'] = re.sub(r'<.*?>', '', location_match[-1]).strip()\n",
    "\n",
    "        experiences.append(experience)\n",
    "    \n",
    "    # Print the extracted experiences\n",
    "    for exp in experiences:\n",
    "        print(exp)\n",
    "else:\n",
    "    print(\"Experience section not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* **Evolution Nordics at <!---->2 yrs<!---->**\n",
      "   * **Duration:** Dec 2023 - Present (8 mos)\n",
      "   * **Location:** <!---->Warrington, England, United Kingdom<!---->\n",
      "\n",
      "* **Auxiliare de conversaci√≥n at <!---->British Council ¬∑ Full-time<!---->**\n",
      "   * **Duration:** Oct 2021 - Aug 2022 (11 mos)\n",
      "   * **Location:** <!---->Murcia, Regi√≥n de Murcia, Spain<!---->\n",
      "\n",
      "* **Customer Service Assistant at <!---->Iceland Foods ¬∑ Part-time<!---->**\n",
      "   * **Duration:** Oct 2020 - Oct 2021 (1 yr 1 mo)\n",
      "   * **Location:** <!---->Liverpool, England, United Kingdom<!---->\n",
      "\n",
      "* **Bartender at <!---->Liverpool Football Club ¬∑ Part-time<!---->**\n",
      "   * **Duration:** Mar 2019 - Oct 2021 (2 yrs 8 mos)\n",
      "   * **Location:** <!---->Liverpool, England, United Kingdom<!---->\n",
      "\n",
      "* **Customer Service Staff at <!---->Mauds Ice Creams Ltd<!---->**\n",
      "   * **Duration:** Feb 2018 - Oct 2020 (2 yrs 9 mos)\n",
      "   * **Location:** <!---->Newtownards, County Down, Northern Ireland<!---->\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def format_duration(duration_str):\n",
    "    parts = duration_str.split(' ¬∑ ')\n",
    "    if len(parts) == 2:\n",
    "        date_range, duration = parts\n",
    "        return f\"{date_range} ({duration})\"\n",
    "    return duration_str\n",
    "\n",
    "with open('html4.txt', 'r', encoding='utf-8') as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "experience_pattern = re.compile(r'<div id=\"experience\".*?</section>', re.DOTALL)\n",
    "experience_section = experience_pattern.search(html_content)\n",
    "\n",
    "if experience_section:\n",
    "    experience_html = experience_section.group()\n",
    "    \n",
    "    item_pattern = re.compile(r'<li class=\"artdeco-list__item.*?</li>', re.DOTALL)\n",
    "    items = item_pattern.findall(experience_html)\n",
    "\n",
    "    experiences = []\n",
    "    for item in items:\n",
    "        experience = {'position': '', 'company': '', 'duration': '', 'location': ''}\n",
    "        \n",
    "        company_match = re.search(r'data-field=\"experience_company_logo\".*?<span aria-hidden=\"true\">(.*?)</span>', item, re.DOTALL)\n",
    "        if company_match:\n",
    "            experience['position'] = re.sub(r'<.*?>', '', company_match.group(1)).strip()\n",
    "        \n",
    "        position_matches = re.findall(r'<span aria-hidden=\"true\">(.*?)</span>', item)\n",
    "        if position_matches:\n",
    "            experience['company'] = position_matches[1].strip()  # The second match should be the position\n",
    "        \n",
    "        duration_match = re.search(r'<span class=\"pvs-entity__caption-wrapper\" aria-hidden=\"true\">(.*?)</span>', item)\n",
    "        if duration_match:\n",
    "            experience['duration'] = re.sub(r'<.*?>', '', duration_match.group(1)).strip()\n",
    "        \n",
    "        location_matches = re.findall(r'<span aria-hidden=\"true\">(.*?)</span>', item)\n",
    "        if len(location_matches) > 2:\n",
    "            experience['location'] = location_matches[-1].strip()  # The last match should be the location\n",
    "\n",
    "        experiences.append(experience)\n",
    "    \n",
    "    for exp in experiences:\n",
    "        print(f\"* **{exp['position']} at {exp['company']}**\")\n",
    "        if exp['duration']:\n",
    "            print(f\"   * **Duration:** {format_duration(exp['duration'])}\")\n",
    "        if exp['location']:\n",
    "            print(f\"   * **Location:** {exp['location']}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"Experience section not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Profile Details:\n",
      "Full name: Sian Vance\n",
      "Headline: Data Recruitment Consultant (Nordicsüá©üá∞üêø) at Evolution Recruitment Solutions - We get your projects delivered through the provision of Tech Freelancers | Evolution Exchange Podcast Host\n",
      "Location: Liverpool, England, United Kingdom\n",
      "\n",
      "About Section:\n",
      "I connect exciting Nordics tech companies with world-leading freelance talent. Underpinning my talent acquisition strategy are three main pillars:Contribution ‚Äì I speak to Tech Leaders every day about the challenges they face and the insights they‚Äôve gained from the market ‚Äì insights that, up until now, were left unshared. Through my podcast platform, I‚Äôm sitting down with business leaders and sharing these insights with my network of freelancers. Check out my featured section for a sample of my latest content.Community ‚Äì I‚Äôm proud to host the Evolution Exchange Podcast Nordics ‚Äì one of the Nordics fastest growing tech podcasts. With well over 250 daily listeners, the Evolution Exchange Podcast Nordics is a hive of discussion where the best companies and freelancers come together to exchange tips, tricks and advice.Collaboration ‚Äì I work with an incredible team of talented and ambitious recruiters who are specialists in sourcing people for agile development teams.If you would like to contribute your insights with our growing community of Copenhagen based freelancers, or are looking for your next assignment, click connect.I connect exciting Nordics tech companies with world-leading freelance talent. Underpinning my talent acquisition strategy are three main pillars:\n",
      "\n",
      "Contribution ‚Äì I speak to Tech Leaders every day about the challenges they face and the insights they‚Äôve gained from the market ‚Äì insights that, up until now, were left unshared. Through my podcast platform, I‚Äôm sitting down with business leaders and sharing these insights with my network of freelancers. Check out my featured section for a sample of my latest content.\n",
      " \n",
      "Community ‚Äì I‚Äôm proud to host the Evolution Exchange Podcast Nordics ‚Äì one of the Nordics fastest growing tech podcasts. With well over 250 daily listeners, the Evolution Exchange Podcast Nordics is a hive of discussion where the best companies and freelancers come together to exchange tips, tricks and advice.\n",
      "\n",
      "Collaboration ‚Äì I work with an incredible team of talented and ambitious recruiters who are specialists in sourcing people for agile development teams. \n",
      "\n",
      "If you would like to contribute your insights with our growing community of Copenhagen based freelancers, or are looking for your next assignment, click connect.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Read the HTML content from the file\n",
    "with open('html4.txt', 'r', encoding='utf-8') as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# Extract full name using regex\n",
    "full_name_match = re.search(r'(?<=<title>).+? \\| LinkedIn', html_content)\n",
    "full_name = full_name_match.group().replace(\" | LinkedIn\", \"\") if full_name_match else \"Full name not found\"\n",
    "\n",
    "# Extract headline using regex\n",
    "headline_match = re.search(r'<div class=\"text-body-medium break-words\"[^>]*>([^<]+)</div>', html_content)\n",
    "headline = headline_match.group(1).strip() if headline_match else \"Headline not found\"\n",
    "\n",
    "# Extract location using regex\n",
    "location_match = re.search(r'<span class=\"text-body-small inline t-black--light break-words\"[^>]*>([^<]+)</span>', html_content)\n",
    "location = location_match.group(1).strip() if location_match else \"Location not found\"\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Find the \"About\" section more specifically\n",
    "about_header = soup.find('div', {'id': 'about'})\n",
    "\n",
    "# Initialize the \"About\" section text\n",
    "about_text = \"About section not found\"\n",
    "\n",
    "# Ensure we find the specific parent container that contains the \"About\" text\n",
    "if about_header:\n",
    "    about_section = about_header.find_next('div', class_='display-flex ph5 pv3')\n",
    "    if about_section:\n",
    "        # Extract the text content from each child element separately\n",
    "        about_text = \"\"\n",
    "        for child in about_section.children:\n",
    "            if child.name is not None:  # Exclude NavigableString objects\n",
    "                text = child.get_text(strip=True)\n",
    "                if text and text not in about_text:  # Avoid duplicates\n",
    "                    about_text += text + \"\\n\"\n",
    "        about_text = about_text.strip()\n",
    "\n",
    "# Print the extracted details\n",
    "print(\"Extracted Profile Details:\")\n",
    "print(f\"Full name: {full_name}\")\n",
    "print(f\"Headline: {headline}\")\n",
    "print(f\"Location: {location}\")\n",
    "print(\"\\nAbout Section:\")\n",
    "print(about_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Profile Details:\n",
      "Full name: Sian Vance\n",
      "Headline: Data Recruitment Consultant (Nordicsüá©üá∞üêø) at Evolution Recruitment Solutions - We get your projects delivered through the provision of Tech Freelancers | Evolution Exchange Podcast Host\n",
      "Location: Liverpool, England, United Kingdom\n",
      "\n",
      "About Section:\n",
      "I connect exciting Nordics tech companies with world-leading freelance talent. Underpinning my talent acquisition strategy are three main pillars:Contribution ‚Äì I speak to Tech Leaders every day about the challenges they face and the insights they‚Äôve gained from the market ‚Äì insights that, up until now, were left unshared. Through my podcast platform, I‚Äôm sitting down with business leaders and sharing these insights with my network of freelancers. Check out my featured section for a sample of my latest content.Community ‚Äì I‚Äôm proud to host the Evolution Exchange Podcast Nordics ‚Äì one of the Nordics fastest growing tech podcasts. With well over 250 daily listeners, the Evolution Exchange Podcast Nordics is a hive of discussion where the best companies and freelancers come together to exchange tips, tricks and advice.Collaboration ‚Äì I work with an incredible team of talented and ambitious recruiters who are specialists in sourcing people for agile development teams.If you would like to contribute your insights with our growing community of Copenhagen based freelancers, or are looking for your next assignment, click connect.I connect exciting Nordics tech companies with world-leading freelance talent. Underpinning my talent acquisition strategy are three main pillars:\n",
      "\n",
      "Contribution ‚Äì I speak to Tech Leaders every day about the challenges they face and the insights they‚Äôve gained from the market ‚Äì insights that, up until now, were left unshared. Through my podcast platform, I‚Äôm sitting down with business leaders and sharing these insights with my network of freelancers. Check out my featured section for a sample of my latest content.\n",
      " \n",
      "Community ‚Äì I‚Äôm proud to host the Evolution Exchange Podcast Nordics ‚Äì one of the Nordics fastest growing tech podcasts. With well over 250 daily listeners, the Evolution Exchange Podcast Nordics is a hive of discussion where the best companies and freelancers come together to exchange tips, tricks and advice.\n",
      "\n",
      "Collaboration ‚Äì I work with an incredible team of talented and ambitious recruiters who are specialists in sourcing people for agile development teams. \n",
      "\n",
      "If you would like to contribute your insights with our growing community of Copenhagen based freelancers, or are looking for your next assignment, click connect.\n",
      "\n",
      "Experience:\n",
      "Evolution Nordics at 2 yrs\n",
      "   Duration: Dec 2023 - Present (8 mos)\n",
      "   Location: Warrington, England, United Kingdom\n",
      "\n",
      "Auxiliare de conversaci√≥n at British Council ¬∑ Full-time\n",
      "   Duration: Oct 2021 - Aug 2022 (11 mos)\n",
      "   Location: Murcia, Regi√≥n de Murcia, Spain\n",
      "\n",
      "Customer Service Assistant at Iceland Foods ¬∑ Part-time\n",
      "   Duration: Oct 2020 - Oct 2021 (1 yr 1 mo)\n",
      "   Location: Liverpool, England, United Kingdom\n",
      "\n",
      "Bartender at Liverpool Football Club ¬∑ Part-time\n",
      "   Duration: Mar 2019 - Oct 2021 (2 yrs 8 mos)\n",
      "   Location: Liverpool, England, United Kingdom\n",
      "\n",
      "Customer Service Staff at Mauds Ice Creams Ltd\n",
      "   Duration: Feb 2018 - Oct 2020 (2 yrs 9 mos)\n",
      "   Location: Newtownards, County Down, Northern Ireland\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to clean unwanted patterns\n",
    "def clean_text(text):\n",
    "    return re.sub(r'<!---->', '', text)\n",
    "\n",
    "# Read the HTML content from the file\n",
    "with open('html4.txt', 'r', encoding='utf-8') as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# Extract full name using regex\n",
    "full_name_match = re.search(r'(?<=<title>).+? \\| LinkedIn', html_content)\n",
    "full_name = full_name_match.group().replace(\" | LinkedIn\", \"\") if full_name_match else \"Full name not found\"\n",
    "full_name = clean_text(full_name)\n",
    "\n",
    "# Extract headline using regex\n",
    "headline_match = re.search(r'<div class=\"text-body-medium break-words\"[^>]*>([^<]+)</div>', html_content)\n",
    "headline = headline_match.group(1).strip() if headline_match else \"Headline not found\"\n",
    "headline = clean_text(headline)\n",
    "\n",
    "# Extract location using regex\n",
    "location_match = re.search(r'<span class=\"text-body-small inline t-black--light break-words\"[^>]*>([^<]+)</span>', html_content)\n",
    "location = location_match.group(1).strip() if location_match else \"Location not found\"\n",
    "location = clean_text(location)\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Find the \"About\" section more specifically\n",
    "about_header = soup.find('div', {'id': 'about'})\n",
    "\n",
    "# Initialize the \"About\" section text\n",
    "about_text = \"About section not found\"\n",
    "\n",
    "# Ensure we find the specific parent container that contains the \"About\" text\n",
    "if about_header:\n",
    "    about_section = about_header.find_next('div', class_='display-flex ph5 pv3')\n",
    "    if about_section:\n",
    "        # Extract the text content from each child element separately\n",
    "        about_text = \"\"\n",
    "        for child in about_section.children:\n",
    "            if child.name is not None:  # Exclude NavigableString objects\n",
    "                text = child.get_text(strip=True)\n",
    "                if text and text not in about_text:  # Avoid duplicates\n",
    "                    about_text += text + \"\\n\"\n",
    "        about_text = about_text.strip()\n",
    "        about_text = clean_text(about_text)\n",
    "\n",
    "# Extract experience section using regex\n",
    "experience_pattern = re.compile(r'<div id=\"experience\".*?</section>', re.DOTALL)\n",
    "experience_section = experience_pattern.search(html_content)\n",
    "\n",
    "experiences = []\n",
    "if experience_section:\n",
    "    experience_html = experience_section.group()\n",
    "    \n",
    "    item_pattern = re.compile(r'<li class=\"artdeco-list__item.*?</li>', re.DOTALL)\n",
    "    items = item_pattern.findall(experience_html)\n",
    "\n",
    "    for item in items:\n",
    "        experience = {'position': '', 'company': '', 'duration': '', 'location': ''}\n",
    "        \n",
    "        company_match = re.search(r'data-field=\"experience_company_logo\".*?<span aria-hidden=\"true\">(.*?)</span>', item, re.DOTALL)\n",
    "        if company_match:\n",
    "            experience['position'] = re.sub(r'<.*?>', '', company_match.group(1)).strip()\n",
    "        \n",
    "        position_matches = re.findall(r'<span aria-hidden=\"true\">(.*?)</span>', item)\n",
    "        if position_matches:\n",
    "            experience['company'] = position_matches[1].strip()  # The second match should be the position\n",
    "        \n",
    "        duration_match = re.search(r'<span class=\"pvs-entity__caption-wrapper\" aria-hidden=\"true\">(.*?)</span>', item)\n",
    "        if duration_match:\n",
    "            experience['duration'] = re.sub(r'<.*?>', '', duration_match.group(1)).strip()\n",
    "        \n",
    "        location_matches = re.findall(r'<span aria-hidden=\"true\">(.*?)</span>', item)\n",
    "        if len(location_matches) > 2:\n",
    "            experience['location'] = location_matches[-1].strip()  # The last match should be the location\n",
    "\n",
    "        # Clean the text\n",
    "        experience = {key: clean_text(value) for key, value in experience.items()}\n",
    "        experiences.append(experience)\n",
    "\n",
    "# Print the extracted details\n",
    "print(\"Extracted Profile Details:\")\n",
    "print(f\"Full name: {full_name}\")\n",
    "print(f\"Headline: {headline}\")\n",
    "print(f\"Location: {location}\")\n",
    "print(\"\\nAbout Section:\")\n",
    "print(about_text)\n",
    "print(\"\\nExperience:\")\n",
    "for exp in experiences:\n",
    "    print(f\"{exp['position']} at {exp['company']}\")\n",
    "    if exp['duration']:\n",
    "        print(f\"   Duration: {format_duration(exp['duration'])}\")\n",
    "    if exp['location']:\n",
    "        print(f\"   Location: {exp['location']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Education\n",
      "University: University of Liverpool\n",
      "Degree: Geography BSc\n",
      "Field of Study: Geography\n",
      "Duration: 2018 - 2021\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "with open('html4.txt', 'r', encoding='utf-8') as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# Pattern to match the education section\n",
    "education_pattern = re.compile(r'<div id=\"education\".*?>(.*?)</section>', re.DOTALL)\n",
    "education_section = education_pattern.search(html_content)\n",
    "\n",
    "if education_section:\n",
    "    # Extract the relevant part of the education section\n",
    "    education_html = education_section.group(1)\n",
    "    \n",
    "    # Pattern to match individual education entries\n",
    "    entry_pattern = re.compile(r'<li class=\"artdeco-list__item.*?\">(.*?)</li>', re.DOTALL)\n",
    "    entries = entry_pattern.findall(education_html)\n",
    "    \n",
    "    education_list = []\n",
    "    \n",
    "    for entry in entries:\n",
    "        # Extract university name\n",
    "        university_pattern = re.compile(r'<span aria-hidden=\"true\">(.*?)</span>')\n",
    "        university_match = university_pattern.findall(entry)\n",
    "        \n",
    "        # Extract degree and field of study\n",
    "        degree_pattern = re.compile(r'<span aria-hidden=\"true\">(.*?), (.*?)</span>')\n",
    "        degree_match = degree_pattern.findall(entry)\n",
    "        \n",
    "        # Extract years\n",
    "        years_pattern = re.compile(r'<span class=\"pvs-entity__caption-wrapper\" aria-hidden=\"true\">(.*?)</span>')\n",
    "        years_match = years_pattern.findall(entry)\n",
    "        \n",
    "        if university_match and degree_match and years_match:\n",
    "            university = re.sub(r'<!---->', '', university_match[0]).strip()\n",
    "            degree = re.sub(r'<!---->', '', degree_match[0][0]).strip()\n",
    "            field_of_study = re.sub(r'<!---->', '', degree_match[0][1]).strip()\n",
    "            years = re.sub(r'<!---->', '', years_match[0]).strip()\n",
    "            \n",
    "            education_list.append({\n",
    "                'University': university,\n",
    "                'Degree': degree,\n",
    "                'Field of Study': field_of_study,\n",
    "                'Years': years\n",
    "            })\n",
    "\n",
    "    print(\"Education\")\n",
    "    for education in education_list:\n",
    "        print(f\"University: {education['University']}\")\n",
    "        print(f\"Degree: {education['Degree']}\")\n",
    "        print(f\"Field of Study: {education['Field of Study']}\")\n",
    "        print(f\"Duration: {education['Years']}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"No education section found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Profile Details:\n",
      "Full name: (1) Sian Vance\n",
      "Headline: Data Recruitment Consultant (Nordicsüá©üá∞üêø) at Evolution Recruitment Solutions - We get your projects delivered through the provision of Tech Freelancers | Evolution Exchange Podcast Host\n",
      "Location: Liverpool, England, United Kingdom\n",
      "\n",
      "About Section:\n",
      "I connect exciting Nordics tech companies with world-leading freelance talent. Underpinning my talent acquisition strategy are three main pillars:Contribution ‚Äì I speak to Tech Leaders every day about the challenges they face and the insights they‚Äôve gained from the market ‚Äì insights that, up until now, were left unshared. Through my podcast platform, I‚Äôm sitting down with business leaders and sharing these insights with my network of freelancers. Check out my featured section for a sample of my latest content.Community ‚Äì I‚Äôm proud to host the Evolution Exchange Podcast Nordics ‚Äì one of the Nordics fastest growing tech podcasts. With well over 250 daily listeners, the Evolution Exchange Podcast Nordics is a hive of discussion where the best companies and freelancers come together to exchange tips, tricks and advice.Collaboration ‚Äì I work with an incredible team of talented and ambitious recruiters who are specialists in sourcing people for agile development teams.If you would like to contribute your insights with our growing community of Copenhagen based freelancers, or are looking for your next assignment, click connect.I connect exciting Nordics tech companies with world-leading freelance talent. Underpinning my talent acquisition strategy are three main pillars:\n",
      "\n",
      "Contribution ‚Äì I speak to Tech Leaders every day about the challenges they face and the insights they‚Äôve gained from the market ‚Äì insights that, up until now, were left unshared. Through my podcast platform, I‚Äôm sitting down with business leaders and sharing these insights with my network of freelancers. Check out my featured section for a sample of my latest content.\n",
      " \n",
      "Community ‚Äì I‚Äôm proud to host the Evolution Exchange Podcast Nordics ‚Äì one of the Nordics fastest growing tech podcasts. With well over 250 daily listeners, the Evolution Exchange Podcast Nordics is a hive of discussion where the best companies and freelancers come together to exchange tips, tricks and advice.\n",
      "\n",
      "Collaboration ‚Äì I work with an incredible team of talented and ambitious recruiters who are specialists in sourcing people for agile development teams. \n",
      "\n",
      "If you would like to contribute your insights with our growing community of Copenhagen based freelancers, or are looking for your next assignment, click connect.\n",
      "\n",
      "Experience:\n",
      "Evolution Nordics at 2 yrs\n",
      "   Duration: Dec 2023 - Present ¬∑ 8 mos\n",
      "   Location: Warrington, England, United Kingdom\n",
      "\n",
      "Auxiliare de conversaci√≥n at British Council ¬∑ Full-time\n",
      "   Duration: Oct 2021 - Aug 2022 ¬∑ 11 mos\n",
      "   Location: Murcia, Regi√≥n de Murcia, Spain\n",
      "\n",
      "Customer Service Assistant at Iceland Foods ¬∑ Part-time\n",
      "   Duration: Oct 2020 - Oct 2021 ¬∑ 1 yr 1 mo\n",
      "   Location: Liverpool, England, United Kingdom\n",
      "\n",
      "Bartender at Liverpool Football Club ¬∑ Part-time\n",
      "   Duration: Mar 2019 - Oct 2021 ¬∑ 2 yrs 8 mos\n",
      "   Location: Liverpool, England, United Kingdom\n",
      "\n",
      "Customer Service Staff at Mauds Ice Creams Ltd\n",
      "   Duration: Feb 2018 - Oct 2020 ¬∑ 2 yrs 9 mos\n",
      "   Location: Newtownards, County Down, Northern Ireland\n",
      "\n",
      "\n",
      "Education:\n",
      "University: University of Liverpool\n",
      "Degree: Geography BSc\n",
      "Field of Study: Geography\n",
      "Duration: 2018 - 2021\n",
      "\n",
      "University: Regent House Grammar School\n",
      "Degree: \n",
      "Field of Study: \n",
      "Duration: 2011 - 2018\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to clean unwanted patterns\n",
    "def clean_text(text):\n",
    "    return re.sub(r'<!---->', '', text)\n",
    "\n",
    "# Read the HTML content from the file\n",
    "with open('html4.txt', 'r', encoding='utf-8') as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# Extract full name using regex\n",
    "full_name_match = re.search(r'(?<=<title>).+? \\| LinkedIn', html_content)\n",
    "full_name = full_name_match.group().replace(\" | LinkedIn\", \"\") if full_name_match else \"Full name not found\"\n",
    "full_name = clean_text(full_name)\n",
    "\n",
    "# Extract headline using regex\n",
    "headline_match = re.search(r'<div class=\"text-body-medium break-words\"[^>]*>([^<]+)</div>', html_content)\n",
    "headline = headline_match.group(1).strip() if headline_match else \"Headline not found\"\n",
    "headline = clean_text(headline)\n",
    "\n",
    "# Extract location using regex\n",
    "location_match = re.search(r'<span class=\"text-body-small inline t-black--light break-words\"[^>]*>([^<]+)</span>', html_content)\n",
    "location = location_match.group(1).strip() if location_match else \"Location not found\"\n",
    "location = clean_text(location)\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Find the \"About\" section more specifically\n",
    "about_header = soup.find('div', {'id': 'about'})\n",
    "\n",
    "# Initialize the \"About\" section text\n",
    "about_text = \"About section not found\"\n",
    "\n",
    "# Ensure we find the specific parent container that contains the \"About\" text\n",
    "if about_header:\n",
    "    about_section = about_header.find_next('div', class_='display-flex ph5 pv3')\n",
    "    if about_section:\n",
    "        # Extract the text content from each child element separately\n",
    "        about_text = \"\"\n",
    "        for child in about_section.children:\n",
    "            if child.name is not None:  # Exclude NavigableString objects\n",
    "                text = child.get_text(strip=True)\n",
    "                if text and text not in about_text:  # Avoid duplicates\n",
    "                    about_text += text + \"\\n\"\n",
    "        about_text = about_text.strip()\n",
    "        about_text = clean_text(about_text)\n",
    "\n",
    "# Extract experience section using regex\n",
    "experience_pattern = re.compile(r'<div id=\"experience\".*?</section>', re.DOTALL)\n",
    "experience_section = experience_pattern.search(html_content)\n",
    "\n",
    "experiences = []\n",
    "if experience_section:\n",
    "    experience_html = experience_section.group()\n",
    "    \n",
    "    item_pattern = re.compile(r'<li class=\"artdeco-list__item.*?</li>', re.DOTALL)\n",
    "    items = item_pattern.findall(experience_html)\n",
    "\n",
    "    for item in items:\n",
    "        experience = {'position': '', 'company': '', 'duration': '', 'location': ''}\n",
    "        \n",
    "        company_match = re.search(r'data-field=\"experience_company_logo\".*?<span aria-hidden=\"true\">(.*?)</span>', item, re.DOTALL)\n",
    "        if company_match:\n",
    "            experience['position'] = re.sub(r'<.*?>', '', company_match.group(1)).strip()\n",
    "        \n",
    "        position_matches = re.findall(r'<span aria-hidden=\"true\">(.*?)</span>', item)\n",
    "        if position_matches:\n",
    "            experience['company'] = position_matches[1].strip()  # The second match should be the position\n",
    "        \n",
    "        duration_match = re.search(r'<span class=\"pvs-entity__caption-wrapper\" aria-hidden=\"true\">(.*?)</span>', item)\n",
    "        if duration_match:\n",
    "            experience['duration'] = re.sub(r'<.*?>', '', duration_match.group(1)).strip()\n",
    "        \n",
    "        location_matches = re.findall(r'<span aria-hidden=\"true\">(.*?)</span>', item)\n",
    "        if len(location_matches) > 2:\n",
    "            experience['location'] = location_matches[-1].strip()  # The last match should be the location\n",
    "\n",
    "        # Clean the text\n",
    "        experience = {key: clean_text(value) for key, value in experience.items()}\n",
    "        experiences.append(experience)\n",
    "\n",
    "# Extract education section using regex\n",
    "education_pattern = re.compile(r'<div id=\"education\".*?</section>', re.DOTALL)\n",
    "education_section = education_pattern.search(html_content)\n",
    "\n",
    "educations = []\n",
    "if education_section:\n",
    "    education_html = education_section.group()\n",
    "    \n",
    "    entry_pattern = re.compile(r'<li class=\"artdeco-list__item.*?</li>', re.DOTALL)\n",
    "    entries = entry_pattern.findall(education_html)\n",
    "    \n",
    "    for entry in entries:\n",
    "        education = {'university': '', 'degree': '', 'field_of_study': '', 'years': ''}\n",
    "        \n",
    "        university_match = re.search(r'<span aria-hidden=\"true\">(.*?)</span>', entry)\n",
    "        if university_match:\n",
    "            education['university'] = clean_text(university_match.group(1)).strip()\n",
    "        \n",
    "        degree_field_match = re.search(r'<span aria-hidden=\"true\">(.*?), (.*?)</span>', entry)\n",
    "        if degree_field_match:\n",
    "            education['degree'] = clean_text(degree_field_match.group(1)).strip()\n",
    "            education['field_of_study'] = clean_text(degree_field_match.group(2)).strip()\n",
    "        \n",
    "        years_match = re.search(r'<span class=\"pvs-entity__caption-wrapper\" aria-hidden=\"true\">(.*?)</span>', entry)\n",
    "        if years_match:\n",
    "            education['years'] = clean_text(years_match.group(1)).strip()\n",
    "        \n",
    "        educations.append(education)\n",
    "\n",
    "# Print the extracted details\n",
    "print(\"Extracted Profile Details:\")\n",
    "print(f\"Full name: {full_name}\")\n",
    "print(f\"Headline: {headline}\")\n",
    "print(f\"Location: {location}\")\n",
    "print(\"\\nAbout Section:\")\n",
    "print(about_text)\n",
    "print(\"\\nExperience:\")\n",
    "for exp in experiences:\n",
    "    print(f\"{exp['position']} at {exp['company']}\")\n",
    "    if exp['duration']:\n",
    "        print(f\"   Duration: {exp['duration']}\")\n",
    "    if exp['location']:\n",
    "        print(f\"   Location: {exp['location']}\")\n",
    "    print()\n",
    "\n",
    "print(\"\\nEducation:\")\n",
    "for edu in educations:\n",
    "    print(f\"University: {edu['university']}\")\n",
    "    print(f\"Degree: {edu['degree']}\")\n",
    "    print(f\"Field of Study: {edu['field_of_study']}\")\n",
    "    print(f\"Duration: {edu['years']}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profile picture URL 1: https://media.licdn.com/dms/image/D5603AQHzIDhr7wL3kQ/profile-displayphoto-shrink_100_100/0/1707400798206?e=1725494400&v=beta&t=DtunMd-46wSaE_v8vY9fyDBH-pGDd4w0kswWx2LDzSM\n",
      "Profile picture URL 2: https://media.licdn.com/dms/image/C4E03AQEEI0XBRC4kYg/profile-displayphoto-shrink_100_100/0/1661347503453?e=1725494400&v=beta&t=3gkg9U0GUh4bxpea9QfjsYt2rP4P0LYyJfgQffflZpw\n",
      "Profile picture URL 3: https://media.licdn.com/dms/image/C4E03AQEEI0XBRC4kYg/profile-displayphoto-shrink_400_400/0/1661347503453?e=1725494400&v=beta&t=mYhvyptjo9GtVCSyR6RU74yIhy1LTPTrAY36Z1e_r_g\n",
      "Profile picture URL 4: https://media.licdn.com/dms/image/C4D03AQFGWx8T5nWjqQ/profile-displayphoto-shrink_100_100/0/1626346872022?e=1725494400&v=beta&t=ysmKwF4ODmKX3V0N_xelRuyEx2ACLNEP1iCI_gJp9qs\n",
      "Profile picture URL 5: https://media.licdn.com/dms/image/D4D03AQEoZugmAU4vYA/profile-displayphoto-shrink_100_100/0/1665689869058?e=1725494400&v=beta&t=oB3ySTTm3UI2yryOuGCA8YDL3fYb7ciaidoHI0v5V7c\n",
      "Profile picture URL 6: https://media.licdn.com/dms/image/D4E03AQETBcnV8cA1aA/profile-displayphoto-shrink_100_100/0/1673875397835?e=1725494400&v=beta&t=XXjn4frleo5QZJH-jzCcsSjw772xZY2SIFUmQbg25GQ\n",
      "Profile picture URL 7: https://media.licdn.com/dms/image/D4E03AQETBcnV8cA1aA/profile-displayphoto-shrink_100_100/0/1673875397835?e=1725494400&v=beta&t=XXjn4frleo5QZJH-jzCcsSjw772xZY2SIFUmQbg25GQ\n",
      "Profile picture URL 8: https://media.licdn.com/dms/image/C5603AQHpelrydJJlIw/profile-displayphoto-shrink_100_100/0/1578170698770?e=1725494400&v=beta&t=wTNCZB-WKLGWJQwrKXkz9dvg9JZQ7pRn1ElUBhN62-I\n",
      "Profile picture URL 9: https://media.licdn.com/dms/image/C4D03AQERYIKvsyOfdQ/profile-displayphoto-shrink_100_100/0/1516234785154?e=1725494400&v=beta&t=5i996UgEw6n8NcKrA-jRhpuS5RiLhZ3ARfRccKAFe8A\n",
      "Profile picture URL 10: https://media.licdn.com/dms/image/D4E03AQGA8OC28cZmKw/profile-displayphoto-shrink_100_100/0/1720021197652?e=1725494400&v=beta&t=vZwdrf3u4nVrPnXhb7OZhEVkxlyahxv0joYqBJGTL2Q\n",
      "Profile picture URL 11: https://media.licdn.com/dms/image/C4D03AQFygfaoUwNivw/profile-displayphoto-shrink_100_100/0/1619079519835?e=1725494400&v=beta&t=rDAjPqueiDxyIrvmySz-Z0g6CnkkDKXk4UUzIrmsst0\n",
      "Profile picture URL 12: https://media.licdn.com/dms/image/D5603AQF_RD8LuQOJBw/profile-displayphoto-shrink_100_100/0/1701693215303?e=1725494400&v=beta&t=O_UzTTBDtDRuWSXjicxPs-uweRxUnp-kMV8BxMZjyxk\n",
      "Profile picture URL 13: https://media.licdn.com/dms/image/C5603AQHpelrydJJlIw/profile-displayphoto-shrink_100_100/0/1578170698770?e=1725494400&v=beta&t=wTNCZB-WKLGWJQwrKXkz9dvg9JZQ7pRn1ElUBhN62-I\n",
      "Profile picture URL 14: https://media.licdn.com/dms/image/D4E03AQGh89LhgsloRg/profile-displayphoto-shrink_100_100/0/1698924542237?e=1725494400&v=beta&t=HrGjT4-K0ccYevphiw4pQmmvktmfVWup9lRneIXwW6w\n",
      "Profile picture URL 15: https://media.licdn.com/dms/image/C4E03AQFNwxXNZMp2oQ/profile-displayphoto-shrink_100_100/0/1607437533878?e=1725494400&v=beta&t=6qNuIKQsJyM0yB3PW2kaE3tLu1gO8MWBmLqi2eQMlRA\n",
      "Profile picture URL 16: https://media.licdn.com/dms/image/D4E03AQH839KNlSn2TQ/profile-displayphoto-shrink_100_100/0/1682175369290?e=1725494400&v=beta&t=1RgyAFomnhLoL96WhGBOejGxGrycGsB5FJn-HlPkXK8\n",
      "Profile picture URL 17: https://media.licdn.com/dms/image/D4E03AQENV-7q5blgOQ/profile-displayphoto-shrink_100_100/0/1696370172559?e=1725494400&v=beta&t=WeRsc3Yl3R4bEM4gv7KRInuX_tsThAx3HPqDNPP_uns\n",
      "Profile picture URL 18: https://media.licdn.com/dms/image/C4E03AQE0snRb1oGDSQ/profile-displayphoto-shrink_100_100/0/1517271347208?e=1725494400&v=beta&t=XV7om-ZUP2vhkjzv6TQDWIGRzxZCs0O2x5ZregaEWgU\n",
      "Profile picture URL 19: https://media.licdn.com/dms/image/C5603AQE1eQWAWYNzcg/profile-displayphoto-shrink_100_100/0/1562843184799?e=1725494400&v=beta&t=HebxbEV8hfA8IIcortepK0C6O200QmpRP-WqSiO0qRw\n",
      "Profile picture URL 20: https://media.licdn.com/dms/image/C4D03AQG0_mEbnVp-bg/profile-displayphoto-shrink_100_100/0/1640257347492?e=1725494400&v=beta&t=ypdom6FV1LKZ-iY038UsyusydqSfJhrg-hD7LErV0SY\n",
      "Profile picture URL 21: https://media.licdn.com/dms/image/C4E03AQGz7YTDrwpSmw/profile-displayphoto-shrink_100_100/0/1613994084634?e=1725494400&v=beta&t=LFdKXdVNzP8ofSUa9ngYcVh8c7kAjYgNaWZavhcArEw\n",
      "Profile picture URL 22: https://media.licdn.com/dms/image/D5603AQHy5PhnL2JeEA/profile-displayphoto-shrink_100_100/0/1714242326332?e=1725494400&v=beta&t=bYh9eZHEYRhWXu1fEAAps5Q2FOGQf7p4z-I58_tbI3A\n",
      "Profile picture URL 23: https://media.licdn.com/dms/image/D4D03AQHy2hcYJGnHkw/profile-displayphoto-shrink_100_100/0/1718239320143?e=1725494400&v=beta&t=RKntKsOtvF3_tDNMnliph--jytTcmx7WPShE9pBvj6s\n",
      "Profile picture URL 24: https://media.licdn.com/dms/image/D4D03AQHxQNOXRHfOAQ/profile-displayphoto-shrink_100_100/0/1715526618656?e=1725494400&v=beta&t=-H1e90CADFMvs6wtoULpRB5ZCjyzliUXkaTVrNV6zqw\n",
      "Profile picture URL 25: https://media.licdn.com/dms/image/D5603AQEUBbs_ps7UQw/profile-displayphoto-shrink_100_100/0/1695746277821?e=1725494400&v=beta&t=XJ_FUsHj8DS0gRXV_3xDbPD_wgiu1aPF5i9VvuO45OI\n",
      "Profile picture URL 26: https://media.licdn.com/dms/image/D5603AQFA8IpB4kXEXQ/profile-displayphoto-shrink_100_100/0/1718297313898?e=1725494400&v=beta&t=H7rAijAMr5tEz5R26VHBOXXtOp7TJbNoMiRJiYf4HYM\n",
      "Profile picture URL 27: https://media.licdn.com/dms/image/D5603AQHzIDhr7wL3kQ/profile-displayphoto-shrink_100_100/0/1707400798206?e=1725494400&v=beta&t=DtunMd-46wSaE_v8vY9fyDBH-pGDd4w0kswWx2LDzSM\n",
      "Profile picture URL 28: https://media.licdn.com/dms/image/D5603AQHoPtQ-6x9V8A/profile-displayphoto-shrink_100_100/0/1718277934681?e=1725494400&v=beta&t=LeKG6rSwqm0D-j9xYoTz29i3lI_XraKnL0rsQ_GMyPI\n",
      "Profile picture URL 29: https://media.licdn.com/dms/image/D4D03AQF61mTD6rwycg/profile-displayphoto-shrink_100_100/0/1695311217731?e=1725494400&v=beta&t=-QM0Z6Td5OvTi0opwSwGrOV9zetLt342j1KL-_z-O7g\n",
      "Profile picture URL 30: https://media.licdn.com/dms/image/C4D03AQHFot31JK1Rhw/profile-displayphoto-shrink_100_100/0/1578354889038?e=1725494400&v=beta&t=-j5oE5IxfKJbUT3Bn6plljD5LM69sfitv0Zl_c-x6hU\n",
      "Profile picture URL 31: https://media.licdn.com/dms/image/C4E03AQEEI0XBRC4kYg/profile-displayphoto-shrink_100_100/0/1661347503453?e=1725494400&v=beta&t=3gkg9U0GUh4bxpea9QfjsYt2rP4P0LYyJfgQffflZpw\n",
      "Profile picture URL 32: https://media.licdn.com/dms/image/D4E03AQGIEDjdY5k7og/profile-displayphoto-shrink_100_100/0/1719835208894?e=1725494400&v=beta&t=N0O2SRc3SaODQb7JhYVPoG42lliJC_6Cy2blf5fCEyc\n",
      "Profile picture URL 33: https://media.licdn.com/dms/image/D4E03AQFKf8yxGj-FPw/profile-displayphoto-shrink_100_100/0/1714980756789?e=1725494400&v=beta&t=TmagP-mhho7XTgWjteEeeWtREunVAQyO90K8FR_v9Qo\n",
      "Profile picture URL 34: https://media.licdn.com/dms/image/D4D03AQHP1w8-jHZr3g/profile-displayphoto-shrink_100_100/0/1686988884612?e=1725494400&v=beta&t=ut2J9Pg0XvaY-SoxK6cJtaVVvoxcj5HBLU43Y4p0cZ0\n",
      "Profile picture URL 35: https://media.licdn.com/dms/image/D4D03AQEBNc2q0gTLsw/profile-displayphoto-shrink_100_100/0/1713786796658?e=1725494400&v=beta&t=ElAmN9wz_km4VRwusx2zX4Q5xtq75ZTMbQGtlq69LnQ\n",
      "Profile picture URL 36: https://media.licdn.com/dms/image/D5603AQGnxAyc0g0wQQ/profile-displayphoto-shrink_100_100/0/1719471565348?e=1725494400&v=beta&t=UNg01qiqH8q4QH8FB9rdyf5STDuXFYhLY5PyAutjStY\n",
      "Profile picture URL 37: https://media.licdn.com/dms/image/D5603AQF_C2KeZY8Cuw/profile-displayphoto-shrink_100_100/0/1702447517433?e=1725494400&v=beta&t=maY6E_9aTodVPxg7ObrWIAhW_fQtb1A_0GHU65cwsME\n",
      "Profile picture URL 38: https://media.licdn.com/dms/image/D5603AQHZSISqpGz_ZA/profile-displayphoto-shrink_100_100/0/1672305346624?e=1725494400&v=beta&t=eQ5d3kFIq9C5Z0L14oMO6mcKNKXor9SHfjzHyvYneqs\n",
      "Profile picture URL 39: https://media.licdn.com/dms/image/D4D03AQGU2lPpx50FAw/profile-displayphoto-shrink_100_100/0/1712658250939?e=1725494400&v=beta&t=SZL0zhpZ847rHbG2oD3GO4ZKYz7tBHvShp7rP3j_Q4Q\n",
      "Profile picture URL 40: https://media.licdn.com/dms/image/D5603AQEgBrYNjj4rBQ/profile-displayphoto-shrink_100_100/0/1718351003006?e=1725494400&v=beta&t=1Lq82qGnVfIYH2bod7Gq2JxHirBQXhFin_05oS6z8pc\n",
      "Profile picture URL 41: https://media.licdn.com/dms/image/D4D03AQGNCInpslWhFQ/profile-displayphoto-shrink_100_100/0/1718321251633?e=1725494400&v=beta&t=Y6rrRcAdYYSww4rtlbStQUfzyGUWF__VgFCyxTPJFjU\n",
      "Profile picture URL 42: https://media.licdn.com/dms/image/D4D03AQFGVlDru2BnmQ/profile-displayphoto-shrink_100_100/0/1689178111629?e=1725494400&v=beta&t=v1tV8ev7Ln21E286B4vmuVoJFWd7_GooxQW-U8HcMYY\n",
      "Profile picture URL 43: https://media.licdn.com/dms/image/C4D03AQFo3b6Ib0-BrA/profile-displayphoto-shrink_100_100/0/1654593904342?e=1725494400&v=beta&t=O_stfpaXImn17VSSNmJGDj_onol0UQMG7WTAuscLvCM\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to extract all profile image URLs and manually inspect them\n",
    "def extract_all_profile_images(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    img_tags = soup.find_all('img')  # Find all img tags\n",
    "\n",
    "    profile_images = []\n",
    "    for img_tag in img_tags:\n",
    "        if 'src' in img_tag.attrs and 'profile-displayphoto-shrink' in img_tag['src']:\n",
    "            profile_images.append(img_tag['src'])\n",
    "\n",
    "    return profile_images\n",
    "\n",
    "# Read HTML content from a file\n",
    "file_path = 'html4.txt'\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# Extract all profile image URLs\n",
    "profile_image_urls = extract_all_profile_images(html_content)\n",
    "\n",
    "# Print all found profile image URLs for manual inspection\n",
    "for i, url in enumerate(profile_image_urls):\n",
    "    print(f\"Profile picture URL {i+1}: {url}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct profile picture URL: https://media.licdn.com/dms/image/C4E03AQEEI0XBRC4kYg/profile-displayphoto-shrink_400_400/0/1661347503453?e=1725494400&v=beta&t=mYhvyptjo9GtVCSyR6RU74yIhy1LTPTrAY36Z1e_r_g\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to extract the third profile image URL\n",
    "def extract_third_profile_image(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    img_tags = soup.find_all('img')  # Find all img tags\n",
    "\n",
    "    profile_images = []\n",
    "    for img_tag in img_tags:\n",
    "        if 'src' in img_tag.attrs and 'profile-displayphoto-shrink' in img_tag['src']:\n",
    "            profile_images.append(img_tag['src'])\n",
    "\n",
    "    # Return the third profile image URL if it exists\n",
    "    if len(profile_images) >= 3:\n",
    "        return profile_images[2]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Read HTML content from a file\n",
    "file_path = 'html4.txt'\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# Extract the correct profile image URL\n",
    "correct_profile_image_url = extract_third_profile_image(html_content)\n",
    "\n",
    "if correct_profile_image_url:\n",
    "    print(f\"Correct profile picture URL: {correct_profile_image_url}\")\n",
    "else:\n",
    "    print(\"No correct profile picture found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Background Image URL: https://media.licdn.com/dms/image/D4E16AQGBPxef88LQXg/profile-displaybackgroundimage-shrink_350_1400/0/1689087623949?e=1725494400&v=beta&t=5xuf-DrHnusTLbAtyn1XwhB9iRJ94ehlHpFjxtVEIg8\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Read the HTML content from the file\n",
    "with open('html4.txt', 'r', encoding='utf-8') as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Find the background image URL\n",
    "background_image_tag = soup.find('img', {'class': 'profile-background-image__image'})\n",
    "background_image_url = background_image_tag['src'] if background_image_tag else None\n",
    "\n",
    "# Print the background image URL\n",
    "if background_image_url:\n",
    "    print(\"Background Image URL:\", background_image_url)\n",
    "else:\n",
    "    print(\"Background Image URL not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Read the HTML content from the file\n",
    "with open('html4.txt', 'r', encoding='utf-8') as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Find the skills section\n",
    "skills_section = soup.find(id=\"skills\")\n",
    "skills = skills_section.find_all('li', class_='artdeco-list__item')\n",
    "\n",
    "# Extract and print skills and endorsements\n",
    "for skill in skills:\n",
    "    skill_name = skill.find('span', class_='t-bold').get_text(strip=True)\n",
    "    endorsement_span = skill.find('span', class_='t-14 t-normal t-black')\n",
    "    endorsements = endorsement_span.get_text(strip=True) if endorsement_span else \"No endorsements\"\n",
    "    print(f\"Skill: {skill_name}, Endorsements: {endorsements}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
